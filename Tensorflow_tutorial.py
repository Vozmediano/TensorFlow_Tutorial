# -*- coding: utf-8 -*-
"""Copia de Copia de TENSORFLOW_TUTORIAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iDy7YEw50BhCrxKshGRjSgyIz8fT8-cb

# <u>**INTRODUCCIÓN A TENSORFLOW**</u>

TENSORFLOW

TensorFlow es una biblioteca de software de código abierto para cómputo numérico, desarrollada por el equipo de Google Brain. Es ampliamente utilizada en el campo del aprendizaje profundo (deep learning), que es una rama del aprendizaje automático (machine learning).

- Como su nombre indica, TensorFlow está diseñado para trabajar con tensores, que son una generalización de vectores y matrices a dimensiones superiores. Esto es fundamental en el aprendizaje profundo, donde se manejan grandes volúmenes de datos en formatos complejos.

- Ofrece APIs de alto nivel como Keras para facilitar la creación y el entrenamiento de modelos de manera intuitiva. Keras actúa como una interfaz en el contexto del aprendizaje profundo. API  quiere decir "Interfaz de Programación de Aplicaciones". Fue diseñada con el objetivo de permitir una experimentación rápida y fácil con redes neuronales, sin necesidad de un conocimiento profundo sobre los aspectos de bajo nivel del cómputo tensorial o la optimización de gradientes.

- TensorFlow se integra bien con otras bibliotecas y herramientas populares en el ecosistema de ciencia de datos y machine learning, como NumPy, Pandas, Matplotlib y Scikit-Learn.

- Es ampliamente utilizado para construir y entrenar redes neuronales para tareas como clasificación de imágenes, procesamiento de lenguaje natural, y muchas otras aplicaciones de IA.

#1 - PERCEPTRON DATASET SINTETICA
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Datos de ejemplo para entrenar la red
# Supongamos que tenemos dos características de entrada (X) y una salida (y)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 0, 0, 1])

# Operación Lógica AND
# La operación lógica AND sigue estas reglas:

# AND de 0 y 0 es 0.
# AND de 0 y 1 es 0.
# AND de 1 y 0 es 0.
# AND de 1 y 1 es 1.

# Creación del modelo de red neuronal de una sola capa
# keras.Sequential(): Esta es la creación de una red neuronal secuencial. En
# Keras, Sequential es un tipo de modelo que se compone de una pila lineal de
# capas. Esto significa que se crea una red neuronal donde cada capa tiene
# exactamente un tensor de entrada y un tensor de salida.

# Dentro del Sequential, hay una lista de capas. En este caso, solo se añade una
# capa, pero podrían añadirse más si se necesitara una red más compleja.

# layers.Dense(units=1, input_shape=(2,), activation='sigmoid'): Esta línea añade
# una capa densa (también conocida como "fully connected") a la red. Vamos a
# desglosar los parámetros de esta capa:

# units=1: Esto define el número de neuronas en la capa. En este caso, hay una
# sola neurona.

# input_shape=(2,): Define la forma del tensor de entrada. Aquí se espera que
# cada entrada tenga 2 características. Esto es importante para la primera capa de
# la red, ya que necesita saber la forma de entrada que se le proporcionará.
# (2,): Es una tupla en Python que indica las dimensiones de los datos de entrada.
# Aquí, solo hay un número en la tupla: 2. Esto significa que cada ejemplo de tus
# datos de entrada debe ser un vector con dos elementos.
# Ejemplos de vectores con 2 elementos. Coordenadas en un plano 2D:
# [3, 4] puede representar un punto en un plano, donde 3 es la coordenada en el
# eje X y 4 es la coordenada en el eje Y.
# [-1, 2] otro punto, donde -1 es la coordenada X y 2 la coordenada Y.

# activation='sigmoid': Esto establece la función de activación a utilizar en esta
# capa. La función de activación 'sigmoid' es comúnmente usada en problemas de
# clasificación binaria, ya que su salida está entre 0 y 1, lo que puede
# interpretarse como una probabilidad.

# En resumen, este código está definiendo una red neuronal muy simple con una sola
# capa densa que tiene una única neurona. Esta red puede tomar entradas de dos
# valores (como se especifica en input_shape) y producirá una salida única entre
# 0 y 1, debido a la función de activación 'sigmoid'. Este tipo de red se podría
# utilizar, por ejemplo, para problemas de clasificación binaria sencillos.

model = keras.Sequential([
    layers.Dense(units=1, input_shape=(2,), activation='sigmoid')
])

# Compilación del modelo
# Esta función configura el modelo para el proceso de entrenamiento. Aquí están
# los componentes principales de compile y su propósito:

# Optimizador (optimizer):
# Define el algoritmo que se usará para actualizar los pesos del modelo
# en función del error (o pérdida) durante el entrenamiento.

# loss
# Mide la discrepancia entre las predicciones del modelo y las
# etiquetas verdaderas. La función de pérdida guía al optimizador sobre cómo
# ajustar los pesos para minimizar el error.

# metrics
# Permiten evaluar el rendimiento del modelo durante el entrenamiento y la
# evaluación. Las métricas no afectan directamente el entrenamiento pero proporcionan
# información sobre el desempeño del modelo.


# 'sgd' se refiere al "Descenso de Gradiente Estocástico" (Stochastic Gradient
# Descent). Es uno de los optimizadores más básicos y ampliamente usados. El SGD
# actualiza los pesos de la red de manera iterativa en función del gradiente del
# error con respecto a los pesos, ayudando a minimizar la función de pérdida (loss).

# Imagina que estás en una montaña y tu objetivo es llegar al punto más bajo del
# valle. La montaña es una representación de tu función de pérdida, y el punto más
# bajo es el mínimo de esta función, es decir, el punto donde tu modelo tiene el
# menor error posible.

# Posición Actual (Pesos de la Red): En esta analogía, tu posición actual en la
# montaña representa los pesos actuales de tu red neuronal.

# Mirar alrededor (Calcular el Gradiente): Para encontrar la dirección hacia el
# valle, miras a tu alrededor para ver en qué dirección está bajando el terreno.
# En el aprendizaje de la red, esto se hace calculando el "gradiente", que es
# básicamente la dirección en la que el error (pérdida) disminuye más rápidamente.

# Dar un Paso (Actualizar los Pesos): Una vez que sabes en qué dirección bajar,
# das un paso en esa dirección. En la red neuronal, esto significa ajustar los
# pesos un poco en la dirección que reduce el error. El tamaño del paso depende de
# un parámetro llamado "tasa de aprendizaje". La tasa de aprendizaje afecta cómo y
# cuándo eficientemente un modelo encuentra el mínimo de la función de pérdida.

# Repetir el Proceso (Iteración): No llegarás al fondo en un solo paso, así que
# repites el proceso, mirando alrededor y dando otro paso, muchas veces. Cada paso
# te lleva más cerca del fondo del valle.

# Llegar al Punto Más Bajo (Minimizar la Pérdida): Continúas este proceso hasta que
# llegas a un punto donde ya no puedes bajar más, lo que indica que has encontrado
# el punto de menor error posible dadas las circunstancias.

# En resumen, el SGD es como un alpinista que baja por una montaña buscando el
# punto más bajo. En cada paso, el alpinista ajusta su ruta basándose en la
# dirección del descenso más pronunciado. Del mismo modo, en una red neuronal, el
# SGD ajusta iterativamente los pesos para minimizar el error.

model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])

# Entrenamiento del modelo
model.fit(X, y, epochs=100, verbose=1)

# Predicciones con el modelo entrenado
predictions = model.predict(X)
rounded_predictions = np.round(predictions)

# Resultados
print("Predicciones:")
print(predictions)
print("Predicciones redondeadas:")
print(rounded_predictions)

# Explicación de los Resultados
# Predicción de [0, 0]:

# Valor predicho: 0.34886166
# Valor redondeado: 0
# Esto se ajusta al valor esperado (0) para la operación AND entre 0 y 0.

# Predicción de [0, 1]:

# Valor predicho: 0.60320073
# Valor redondeado: 1
# Aquí, el modelo predijo un valor cercano a 0.6, que al ser redondeado se
# convierte en 1. Sin embargo, el valor esperado para AND de 0 y 1 es 0. Esto indica
# que el modelo aún no ha aprendido perfectamente la operación AND.

# Predicción de [1, 0]:

# Valor predicho: 0.36651823
# Valor redondeado: 0
# El modelo predijo correctamente el valor esperado (0) para la operación AND
# entre 1 y 0.

# Predicción de [1, 1]:

# Valor predicho: 0.62144434
# Valor redondeado: 1
# Aquí, el modelo predijo correctamente el valor esperado (1) para la operación
# AND entre 1 y 1.

# Análisis del Modelo
# El modelo ha capturado parcialmente la lógica AND, pero no perfectamente, ya que
# una de las predicciones es incorrecta antes del redondeo ([0, 1] -> 1). Esto
# puede deberse a varias razones:

# Entrenamiento Insuficiente: Aunque 100 épocas pueden ser suficientes para
# problemas simples, el modelo podría necesitar más épocas para ajustar mejor los
# pesos.

# Modelo Simple: El modelo tiene solo una capa con una neurona, lo cual es una
# arquitectura muy simple. A veces, una red neuronal más compleja podría ser
# necesaria para aprender mejor la tarea.

# Ajustes del Modelo:
# Para mejorar el rendimiento del modelo, podrías experimentar con más épocas de
# entrenamiento, ajustar los hiperparámetros o incluso añadir más capas/neuronas
# a la red.

"""#2 - PERCEPTRON DATASET DIABETES"""

import pandas as pd
import numpy as np
from tensorflow import keras
from matplotlib import pyplot as plt

# Carga del dataset
df = pd.read_csv('diabetes.csv')

# Separación de características y variable de destino
X = df[['Glucose', 'BloodPressure', 'SkinThickness', 'BMI', 'Age']]
y = df['Outcome']

# Normalización de datos
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Construcción del modelo
model = keras.Sequential([
    keras.layers.Dense(units=1, activation='sigmoid', input_shape=(X_scaled.shape[1],))
])

# El input_shape=(X_scaled.shape[1],) le dice a la primera capa densa de su modelo
# que espera entradas con un número indeterminado de filas (ejemplos) y un número
# fijo de columnas igual al número de características en sus datos preprocesados
# (en este caso, 5 columnas del tensor X_scaled).

# Compilación del modelo
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Entrenamiento del modelo
history = model.fit(X_scaled, y, epochs=70, validation_split=0.2)

# Evaluación del modelo
metrics = model.evaluate(X_scaled, y)
print('Precisión:', metrics[1])

# Visualización de curvas de aprendizaje
plt.plot(history.history['loss'], label='Loss de entrenamiento')
plt.plot(history.history['val_loss'], label='Loss de validación')
plt.legend()
plt.show()

"""#3 - RED NEURONAL DE 2 CAPAS

"""

# En este ejemplo, se utiliza un modelo simple de red neuronal con dos capas
# densas (fully connected layers) para resolver el problema XOR.
# El problema XOR (o exclusivo) en el contexto del aprendizaje profundo
# (deep learning) es un caso clásico que se utiliza para ilustrar la importancia
# de las redes neuronales con múltiples capas (también conocidas como redes
# neuronales profundas).

# El problema XOR es un problema de clasificación simple. La operación XOR es una
# operación lógica que produce un verdadero (1) solo si el número de entradas
# verdaderas (1s) es impar. En términos más simples, en el caso de dos entradas,
# XOR devuelve verdadero si una de las dos entradas es verdadera y la otra es falsa.

# Aquí está la tabla de verdad para XOR:

# Entrada A /	Entrada B	/ Salida XOR
# 0	             0	           0   FALSO
# 0	             1	           1   VERDADERO
# 1	             0	           1   VERDADERO
# 1	             1	           0   FALSO
# # Si intentas graficar estos puntos en un plano bidimensional (donde cada eje
# representa una de las entradas), notarás que no puedes separar las salidas
# verdaderas de las falsas usando una sola línea recta. Esto significa que el
# problema XOR no es linealmente separable.

# En el contexto del aprendizaje profundo, este problema se convierte en
# significativo porque muestra la limitación de las redes neuronales con una sola
# capa (también conocidas como perceptrones). Un perceptrón simple, que solo puede
# aprender fronteras de decisión lineales, no puede resolver el problema XOR.

# La solución a este problema vino con la introducción de redes neuronales
# multicapa, también conocidas como perceptrones multicapa. Al agregar al menos una
# capa oculta, la red puede aprender fronteras de decisión no lineales, lo que le
# permite resolver el problema XOR. Este descubrimiento fue un paso crucial en el
# desarrollo del campo del aprendizaje profundo, demostrando la importancia de las
# arquitecturas de redes profundas para abordar problemas complejos y no lineales.

# Los datos de entrenamiento consisten en las entradas [0, 0], [0, 1], [1, 0], [1, 1]
# y las salidas esperadas [0, 1, 1, 0]. El modelo se entrena durante 1000 épocas
# utilizando el optimizador Adam y la función de pérdida binary crossentropy.

# Una vez entrenado, el modelo se evalúa utilizando los mismos datos de TEST y
# se muestra la precisión obtenida.

import matplotlib.pyplot as plt

# Valores de entrada y salida para el problema XOR
X = [[0, 0], [0, 1], [1, 0], [1, 1]]  # Entradas
Y = [0, 1, 1, 0]                      # Salidas

# Separando los puntos según su salida
# zip(X, Y) combina X y Y en pares, de modo que cada par consiste en un elemento
# de X y el elemento correspondiente de Y en la misma posición.

# X_0 =[x for x, y in zip(X, Y) if y == 0] es una comprensión de lista, una forma
# concisa de crear listas en Python. Esta línea en particular está creando una
# nueva lista (X_0) que contiene todos los elementos x de X para los cuales el
# correspondiente elemento y en Y es igual a 0.
# En resumen, X_0 contendrá todos los elementos de X cuyas salidas correspondientes
# en Y son 0.

# X_1 = [x for x, y in zip(X, Y) if y == 1]:
# Esta línea es muy similar a la anterior, pero en lugar de recoger los elementos
# x cuyas salidas y son 0, recoge aquellos cuyas salidas y son 1.
# Al final de esta línea, X_1 contendrá todos los elementos de X cuyas salidas
# correspondientes en Y son 1.

X_0 = [x for x, y in zip(X, Y) if y == 0]
X_1 = [x for x, y in zip(X, Y) if y == 1]

# Dibujando los puntos
# plt.scatter es una función que crea un gráfico de dispersión. Cada punto en el
# gráfico se define por un par de coordenadas (x, y).

# [x[0] for x in X_0]: Esta es una comprensión de lista que toma el primer elemento
# (índice 0) de cada sublista x dentro de la lista X_0. Esto representa todos los
# valores de la coordenada 'x' de los puntos en X_0.

# [x[1] for x in X_0]: De manera similar, esta comprensión de lista toma el segundo
# elemento (índice 1) de cada sublista x dentro de la lista X_0. Esto representa
# todos los valores de la coordenada 'y' de los puntos en X_0.

# color='red': Esto establece el color de los puntos en el gráfico de dispersión a rojo.

# label='0': Esto asigna una etiqueta al conjunto de puntos, que puede ser usada
# para crear una leyenda. En este caso, los puntos en X_0 están etiquetados con '0

plt.figure(figsize=(6, 6))
plt.scatter([x[0] for x in X_0], [x[1] for x in X_0], color='red', label='0')
plt.scatter([x[0] for x in X_1], [x[1] for x in X_1], color='blue', label='1')

# Etiquetas y leyenda
plt.title('Visualización del Problema XOR')
plt.xlabel('Entrada 1')
plt.ylabel('Entrada 2')
plt.legend()
plt.grid(True)
plt.show()

import tensorflow as tf
from tensorflow import keras
import numpy as np

# Datos de entrenamiento
x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)
y_train = np.array([0, 1, 1, 0], dtype=np.float32)

# Datos de prueba
x_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)
y_test = np.array([0, 1, 1, 0], dtype=np.float32)

# Construir el modelo

# Creación de la Red Neuronal Secuencial:

# modelo = keras.Sequential([]): Se crea una instancia del modelo secuencial.
# Los modelos secuenciales en Keras son una pila lineal de capas.
# Añadiendo la Primera Capa Densa:

# keras.layers.Dense(2, input_shape=(2,), activation='relu'): Esta es la primera
# capa del modelo, y es una capa "densa" o completamente conectada. Vamos a
# desglosar los parámetros de esta capa:
# 2 es el número de neuronas (o unidades) en la capa. Esta capa tendrá dos neuronas.
# input_shape=(2,) especifica la forma de los datos de entrada que la red espera.
# En este caso, se espera que cada entrada tenga 2 características. Este parámetro
# es importante porque es la primera capa del modelo y necesita saber la forma del
# tensor de entrada.
# activation='relu' establece la función de activación de las neuronas en esta capa.
# 'ReLU' (Rectified Linear Unit) es una función de activación común que introduce
# no linealidades en el modelo, permitiendo que la red aprenda relaciones más
# complejas (con curvas) en los datos.

# Añadiendo la Segunda Capa Densa:

# keras.layers.Dense(1, activation='sigmoid'): Esta es la segunda y última capa del
# modelo, también densa.
# 1 indica que esta capa tiene una única neurona.
# activation='sigmoid' usa la función de activación sigmoid. La función sigmoid es
# especialmente popular para la capa de salida en problemas de clasificación binaria,
# ya que su salida está en el rango de 0 a 1, que puede interpretarse como una probabilidad.

# En resumen, el código define un modelo de red neuronal con dos capas:
# La primera capa es una capa densa con 2 neuronas y una función de activación ReLU,
# diseñada para procesar entradas con 2 características.
# La segunda capa es una capa densa con 1 neurona y una función de activación sigmoid,
# adecuada para producir una salida de clasificación binaria.
# Este tipo de modelo es común en tareas simples de clasificación binaria, donde se
# intenta predecir dos posibles resultados (como sí/no, verdadero/falso, etc.) a
# partir de entradas con dos características.

modelo = keras.Sequential([
    keras.layers.Dense(2, input_dim=2, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

# Compilar el modelo
modelo.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Entrenar el modelo
modelo.fit(x_train, y_train, epochs=10)

# Evaluar el modelo
# resultado = modelo.evaluate(x_test, y_test) utiliza el método evaluate para
# probar el modelo en el conjunto de datos de prueba.
# modelo.evaluate devuelve una lista de valores. El primer valor (resultado[0])
# es la pérdida en los datos de prueba, y el segundo valor (resultado[1])
# generalmente representa la precisión.
# print("Precisión de la prueba:", resultado[1]) imprime la precisión del modelo
# en los datos de prueba. La precisión es una medida de qué tan bien el modelo
# realiza predicciones correctas. En este contexto, resultado[1] es el valor de la
# precisión.
# En resumen, este código está entrenando un modelo de red neuronal con un conjunto
# de datos de entrenamiento (x_train, y_train), luego prueba el modelo con un
# conjunto de datos de prueba (x_test, y_test), y finalmente imprime la precisión
# del modelo en esos datos de prueba.

resultado = modelo.evaluate(x_test, y_test)
print("Precisión de la prueba:", resultado[1])

# En este caso, un valor de loss de 0.7544 sugiere que la red aún tiene un error
# considerable en sus predicciones.

# En este caso, una accuracy de 0.25 significa que el modelo solo está haciendo
# predicciones correctas el 25% del tiempo.

# Posibles motivos de tan mal desempeño:
# 1.- Arquitectura insuficiente
# 2.- Entrenamiento inadecuado o insuficiente
# 3.- Datos poco adecuados (sin procesar, etc.)

"""##3.1 - RED NEURONAL CON MAS CAPAS"""

from keras.models import Sequential
from keras.layers import Dense

# Crear el modelo
model = Sequential()
model.add(Dense(8, input_dim=2, activation='relu'))  # Primera capa oculta con 8 neuronas
model.add(Dense(1, activation='sigmoid'))  # Capa de salida

# Modelo Secuencial: Se utiliza el modelo Sequential de Keras para definir una pila
# lineal de capas. Este modelo es adecuado para construir una red en la que las
# capas se apilan una tras otra.

# Primera capa
# Capa Densa (Fully Connected): La capa Dense es una capa completamente conectada
# donde cada neurona de esta capa está conectada a todas las neuronas de la capa
# anterior (en este caso, la entrada).
# Neuronas: Tiene 8 neuronas.
# Entrada (input_dim): La entrada tiene 2 dimensiones, lo cual es adecuado para el
# problema XOR que tiene dos entradas.
# Función de Activación: relu (Rectified Linear Unit). Esta función de activación
# introduce no linealidad en el modelo, lo cual es esencial para que la red pueda
# aprender patrones complejos. ReLU devuelve 0 si el input es negativo, y el valor
# del input si es positivo (f(x)=max(0,x)).

# Capa de salida
# Capa Densa: La capa de salida también es completamente conectada.
# Neurona: Tiene solo 1 neurona.
# Función de Activación: sigmoid. La función sigmoid es adecuada para problemas de
# clasificación binaria, ya que transforma la salida en un valor entre 0 y 1,
# interpretado como la probabilidad de que la entrada pertenezca a la clase positiva.


# Compilar el modelo
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Datos XOR
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([0,1,1,0])

# Entrenar el modelo
model.fit(X, y, epochs=1000, verbose=0)

model.summary()

# Evaluar el modelo
loss, accuracy = model.evaluate(X, y)
print(f'Loss: {loss}, Accuracy: {accuracy}')

# El desempeño de esta red con mas capas ha mejorado considerablemente ya que el
# loss ha bajado de 0.75 a 0.4 y el accuracy ha subido de 0.25 a 0.75

"""##3.2 - RED NEURONAL CON MAS CAPAS Y MAS NEURONAS"""

from keras.models import Sequential
from keras.layers import Dense
import numpy as np

# Crear el modelo
model = Sequential()
model.add(Dense(8, input_dim=2, activation='relu'))
model.add(Dense(8, activation='relu'))  # Añadir una segunda capa oculta
model.add(Dense(1, activation='sigmoid'))

# FUNCIONAMIENTO DE LA RED:

# Entrada: El modelo recibe un par de valores binarios (0 o 1) como entrada (por
# ejemplo, las combinaciones de entrada del problema XOR).

# Primera Capa Oculta: La entrada se procesa a través de 8 neuronas, aplicando la
# función de activación ReLU para introducir no linealidad.

# Segunda Capa Oculta: La salida de la primera capa se pasa a través de otras 8
# neuronas, aplicando nuevamente ReLU para seguir transformando los datos de manera
# no lineal.

# Salida: La segunda capa oculta se conecta a una sola neurona de salida, que
# aplica la función de activación sigmoid para proporcionar una probabilidad
# entre 0 y 1, que indica la clase predicha.


# Compilar el modelo
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

# Datos XOR
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([0,1,1,0])

# Entrenar el modelo
model.fit(X, y, epochs=1000, verbose=1)

# Evaluar el modelo
loss, accuracy = model.evaluate(X, y)
print(f'Loss: {loss}, Accuracy: {accuracy}')

"""# 4 - RED RNN. DATASET SENTIMIENTO SOBRE PELICULAS


"""

import numpy as np
from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
from keras.preprocessing import sequence

# max_features: Número máximo de palabras más frecuentes que se utilizarán del
# conjunto de datos. Las palabras se indexan por frecuencia de aparición.
# maxlen: Longitud máxima de las secuencias de texto que se procesarán. Si una
# secuencia es más larga, se truncará; si es más corta, se completará con ceros (padding).

# imdb.load_data(num_words=max_features): Carga el conjunto de datos IMDb con las
# max_features palabras más frecuentes. IMDb es un conjunto de datos de críticas
# de películas, donde cada crítica está representada como una secuencia de índices
# de palabras. Las palabras menos frecuentes se descartan.

# Cargar el dataset
max_features = 20000  # Número de palabras más frecuentes a considerar
maxlen = 80  # Máxima longitud de las secuencias
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)

# El objetivo de este preprocesamiento es asegurar que todas las secuencias de
# texto (listas de enteros) tengan la misma longitud (maxlen). Esto es crucial
# para poder alimentar estos datos en una red neuronal, ya que las redes neuronales
# esperan que los datos de entrada tengan dimensiones uniformes.

# Después de ejecutar estos dos comandos, tanto x_train como x_test serán matrices
# 2D donde cada fila representa una secuencia de longitud maxlen. Las secuencias
# originales más cortas se rellenarán con ceros al principio y las secuencias más
# largas se truncarán para que se ajusten a la longitud especificada.

# Qué hace exactamente pad_sequences?
# Truncar secuencias: Si una secuencia es más larga que maxlen, se truncará
# (se cortará) para que tenga una longitud de maxlen.
# Añadir padding: Si una secuencia es más corta que maxlen, se le añadirán
# ceros al principio para que alcance la longitud de maxlen. Por defecto, el
# padding se añade al principio (padding='pre').
# Ejemplo visual:

# Secuencia original: [1, 2, 3, 4, 5]
# Secuencia truncada: [2, 3, 4, 5] (si maxlen=4)
# Secuencia con padding: [0, 0, 1, 2, 3, 4, 5] (si maxlen=7)

# Preprocesar los datos
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)

# Construir el modelo
model = Sequential()
model.add(Embedding(max_features, 128))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

# Resumen del modelo
# Embedding Layer: Convierte secuencias de índices de palabras en vectores densos.
# 128 del Embedding: Es la dimensión de los vectores que representan cada palabra.
# Es decir, cada palabra se convierte en un vector de 128 números.

# max_features: Es el número máximo de palabras distintas que el modelo considerará.
# En este caso, las 20,000 palabras más frecuentes del conjunto de datos.
# LSTM Layer: Captura las dependencias temporales en las secuencias de texto.
# Tiene 128 neuronas
# Dense Layer: Realiza la clasificación binaria (positivo/negativo) con una
# función de activación sigmoide.

# Compilar el modelo
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.summary()

# Entrenar el modelo
model.fit(x_train, y_train,
          batch_size=32,
          epochs=15,
          validation_data=(x_test, y_test))

# batch_size=32: El modelo procesará 32 muestras antes de actualizar los pesos.
# Este proceso se repite hasta que se han procesado todas las muestras en el
# conjunto de datos de entrenamiento.

# durante cada epoch, el modelo utiliza todas las muestras del conjunto de
# entrenamiento, pero las procesa en lotes de 32 muestras a la vez.

# loss: Disminuye significativamente a lo largo de las épocas, indicando que el
# modelo aprende a predecir mejor con cada iteración.
# accuracy: Aumenta constantemente, llegando a una precisión cercana al 99.77% al
# final del entrenamiento.
# val_loss: Inicialmente muestra un comportamiento similar a la pérdida de
# entrenamiento, pero luego aumenta.
# val_accuracy: Aumenta inicialmente, pero luego se estanca o disminuye ligeramente.

# El modelo parece estar aprendiendo bien a clasificar las críticas de entrenamiento,
# como lo indica la disminución de la pérdida de entrenamiento y el aumento de la
# precisión de entrenamiento.
# Sin embargo, el aumento de la pérdida de validación y el estancamiento de la
# precisión de validación sugieren un posible sobreajuste. El modelo podría estar
# memorizando los datos de entrenamiento específicos en lugar de aprender características
# generales para clasificar críticas nuevas.

"""#5 - CNN. DATASET FASHION MNIST

Fashion-MNIST es un conjunto de datos utilizado comúnmente en el campo del aprendizaje automático y la visión por computadora. Es una variante del conjunto de datos MNIST (Modified National Institute of Standards and Technology) original, que es ampliamente utilizado para ejemplos y pruebas en el campo del aprendizaje automático. Mientras que el MNIST original contiene imágenes de dígitos escritos a mano (0 a 9), Fashion-MNIST se compone de imágenes de artículos de moda(botas, camisetas, etc.).

Características principales de Fashion-MNIST:

Imágenes: Contiene 70,000 imágenes en escala de grises. Cada imagen tiene una resolución de 28x28 píxeles.

Categorías: Hay 10 categorías de ropa y accesorios, como camisetas/tops, pantalones, abrigos, vestidos, zapatillas, etc.

Conjunto de datos dividido: Se divide típicamente en un conjunto de entrenamiento de 60,000 imágenes y un conjunto de prueba de 10,000 imágenes.

Propósito: Fue creado como un reemplazo más desafiante para el conjunto de datos MNIST original. Las categorías de ropa presentan una complejidad mayor en comparación con los dígitos escritos a mano del MNIST.

Uso en TensorFlow: En TensorFlow, se utiliza comúnmente para demostrar algoritmos de clasificación y redes neuronales. Se puede cargar fácilmente utilizando las API de TensorFlow.

Fashion-MNIST es especialmente útil para aquellos que comienzan en el campo del aprendizaje automático, ya que proporciona un conjunto de datos más interesante y desafiante que el MNIST original, pero sigue siendo lo suficientemente pequeño y manejable para experimentos y pruebas rápidas.
"""

import tensorflow as tf
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D

# Cargar datos
(train_images, train_labels), (test_images, test_labels)=fashion_mnist.load_data()

import matplotlib.pyplot as plt

# Mostrar las imagenes en muy baja resolucion
for i in range(10):
    # Reshape the flattened image a 28x28 para su visualizacion
    image = train_images[i].reshape(28, 28)
    plt.imshow(image, cmap='gray')
    plt.show()

# Normalizar los datos
# este código normaliza los datos de las imágenes de entrenamiento y prueba
# dividiéndolos por 255. Esto transforma los valores de los píxeles de un rango
# de 0-255 a un rango de 0-1, facilitando el proceso de aprendizaje del modelo.
# Estas imágenes suelen estar en formato de píxeles, donde cada píxel tiene un
# valor de 0 a 255, representando la intensidad de la luz o el color.

train_images=train_images/255.0
test_images=test_images/255.0

# Resimensionar imagenes para el modelo

# Este código se utiliza para cambiar el formato de dos arrays de imágenes,
# train_images y test_images, a un formato compatible con una red neuronal
# convolucional (CNN).

# Formato original:

# Tanto train_images como test_images se almacenan inicialmente en un formato
# que no es adecuado para el procesamiento por una CNN. Cada imagen es una matriz
# de píxeles gris de 28x28, lo que resulta en un array de 28x28x1. Esto significa
# que cada imagen se representa por un array aplanado de 784 píxeles.

# Matriz original: 28 filas x 28 columnas (784 valores)
# Array aplanado: 1 fila x 784 columnas (784 valores)
# Al aplanar la imagen, se mantiene la información de los píxeles, pero se
# transforma en un formato que la CNN puede procesar.

# Formato redimensionado:
# La función reshape() se utiliza para cambiar la forma de cada array de imagen a
# un formato compatible con una CNN. La nueva forma es (-1, 28, 28, 1). El -1 en
# la primera dimensión significa que esta dimensión debe calcularse dinámicamente
# en función del número de imágenes en el array. Las dimensiones
# restantes (28, 28, 1) representan el ancho, la altura y el número de canales de
# cada imagen.

# (-1, 28, 28, 1) especifica la nueva forma que queremos para el conjunto de datos.
# El -1 es un comodín que le dice a Python que ajuste automáticamente esta
# dimensión en base a las otras dimensiones y al tamaño total del conjunto de
# datos. Esto es útil cuando no sabemos cuántas imágenes hay en train_images,
# pero queremos que todas tengan la misma forma.
# 28, 28 significa que cada imagen será reajustada para tener 28 píxeles de ancho
# por 28 píxeles de alto.
# El 1 al final indica que las imágenes son en escala de grises (un solo canal).
# Si las imágenes fueran a color (RGB), este número sería 3, representando los
# canales rojo, verde y azul.

train_images=train_images.reshape((-1, 28, 28, 1))
test_images=test_images.reshape((-1, 28, 28, 1))

# Cuando no conocemos el numero de imagenes usamos el -1 pero si lo conocemos
# el codigo es:

# Reshape images for CNN input (explicit number of images)
# num_train_images = 60000  # Number of images for training
# num_test_images = 10000    # Number of images for testing

# train_images = train_images.reshape((num_train_images, 28, 28, 1))
# test_images = test_images.reshape((num_test_images, 28, 28, 1))

# Construir el modelo
# El código model define un modelo de red
# neuronal convolucional (CNN) para la tarea de reconocimiento de dígitos escritos
# a mano MNIST. Se construye utilizando la API secuencial de Keras, que permite
# apilar capas de forma secuencial.

# Explicación de las capas:

# Conv2D: Esta capa realiza la convolución, la operación fundamental en las CNN.
# Toma 32 filtros de tamaño 3x3, aplica una función de activación ReLU (unidad
# lineal rectificada) para introducir no linealidad, y especifica la forma de
# entrada de los datos (imágenes en escala de grises de 28x28).

# MaxPooling2D: Esta capa reduce las dimensiones espaciales de los mapas de
# características mediante la realización de una agrupación máxima sobre regiones
# de 2x2. Esto ayuda a reducir la complejidad computacional y el sobreajuste.
# Su función principal es reducir progresivamente el tamaño espacial (ancho y
# alto) de los mapas de características para disminuir la cantidad de parámetros
# y el cómputo en la red, lo que ayuda a prevenir el sobreajuste.
# MaxPooling 2x2" es una operación común en las CNNs que ayuda a reducir el tamaño
# de los mapas de características y a extraer las características más importantes
# (los valores máximos), mejorando así la eficiencia y eficacia del aprendizaje
# de la red. 2x2 quiere decir 2 pixeles por 2 pixeles

# Dropout: Esta capa elimina aleatoriamente el 25% de las neuronas durante el
# entrenamiento, evitando el sobreajuste al hacer que el modelo sea menos sensible
# a las neuronas individuales.

# Conv2D: Otra capa convolucional con 64 filtros de tamaño 3x3 y activación ReLU.

# MaxPooling2D: Otra capa de agrupación máxima para reducir dimensiones.

# Dropout: Otra capa de abandono con una tasa de abandono del 25%.
# l 25% de las neuronas de una capa se desactivan aleatoriamente durante el
# entrenamiento. Esta técnica se utiliza para prevenir el sobreajuste y mejorar
# la generalización del modelo.

# Flatten: Esta capa convierte los mapas de características 2D en un vector 1D,
# preparando los datos para las capas totalmente conectadas. Los mapas de
# características en redes neuronales profundas son representaciones intermedias
# generadas al aplicar filtros a la entrada o a los mapas de características de
# capas anteriores. Estos mapas son esenciales para que la red extraiga y aprenda
# características progresivamente más complejas y útiles de los datos de entrada.
# Los mapas de características en redes neuronales, especialmente en las redes
# neuronales convolucionales (CNNs), se pueden entender como matrices. Sin embargo,
# es importante tener en cuenta que son matrices tridimensionales. Te explicaré
# esto con más detalle:
# los mapas de características en CNNs empiezan como matrices bidimensionales
# para imágenes en escala de grises, pero rápidamente se convierten en estructuras
# tridimensionales a medida que se aplican múltiples filtros, especialmente en el
# procesamiento de imágenes a color y en las capas sucesivas de la red.

# Dense: Esta capa realiza operaciones completamente conectadas (FC), conectando
# esencialmente todas las neuronas de la capa anterior con todas las neuronas de
# esta capa. Tiene 128 neuronas y una función de activación ReLU.

# Dense: La capa final es otra capa FC con 10 neuronas, que representan los 10
# posibles prendas de ropa (0-9). Utiliza una función de activación
# softmax, que aplasta la salida en probabilidades, indicando la probabilidad
# de cada clase.

# En resumen, este modelo CNN utiliza capas convolucionales (Conv2D) para la extracción
# de características, capas de agrupación(MaxPooling2D) para la reducción de la dimensionalidad,
# capas de abandono (Dropout) para evitar el sobreajuste, Las capas "Flatten" en una red
# neuronal convolucional (CNN) tienen un propósito muy específico y crucial. Estas
# capas se utilizan para convertir los mapas de características tridimensionales
# obtenidos de las capas convolucionales y de agrupamiento (MaxPooling2D) en un vector
# unidimensional y capas (Dense) totalmente conectadas para
# la clasificación. Es un modelo bien estructurado adecuado para la tarea MNIST,
# logrando una alta precisión en el reconocimiento de prendas de ropa.

# Las capas Dropout y MaxPooling
# son herramientas valiosas para mejorar su rendimiento y generalización,
# especialmente en tareas de clasificación de imágenes.

# El número 32 especifica el número de filtros de convolución que se aplicarán a
# la imagen de entrada. Cada filtro es una matriz de pesos que se aprende durante
# el entrenamiento.
# Significado: Cada filtro detecta diferentes características en la imagen, como
# bordes, texturas y patrones. Tener más filtros permite detectar una mayor
# variedad de características.
# (3, 3) (kernel_size):

# Tamaño del kernel: (3, 3) especifica el tamaño de cada filtro o kernel. En este
# caso, cada filtro es una matriz de 3x3.
# Operación: El filtro se desliza sobre la imagen de entrada, realizando
# operaciones de convolución para producir mapas de características (feature maps).

# input_shape=(28, 28, 1):

# Forma de entrada: Este parámetro especifica la forma de los datos de entrada a
# esta capa.
# (28, 28): Indica que las imágenes de entrada tienen 28 píxeles de alto y 28
# píxeles de ancho.
# 1: Indica que las imágenes tienen un solo canal, lo que es típico de imágenes
# en escala de grises (blanco y negro).

# maxpooling2D Reducción de dimensionalidad ayuda a disminuir la cantidad de
# parámetros y el costo computacional, además de resaltar las características más
# importantes.

# La capa Flatten convierte los datos de entrada en un vector unidimensional (1D).
# Esto es necesario porque las capas densas (fully connected) requieren una
# entrada en forma de vector 1D, mientras que las capas convolucionales y de
# pooling generan datos en forma de matrices multidimensionales
# (por ejemplo, 2D o 3D).

model=Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
    MaxPooling2D(2,2),  #matriz de 2x2 para reducir dimensionalidad
    Dropout(0.25),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Dropout(0.25),
    Flatten(),
    Dense(128, activation='relu'),  #128 neuronas
    Dense(10, activation='softmax')  #10 neuronas
])

# Compilar el modelo
# El compile es un método que configura el modelo para el entrenamiento. En esta
# etapa, se definen varios parámetros clave que controlan cómo el modelo aprenderá
# de los datos.
# Antes de que el modelo pueda ser entrenado, necesita ser compilado.
# En resumen, este código está preparando un modelo de aprendizaje automático
# para el entrenamiento. Selecciona 'Adam' como el método de optimización, usa la
# 'sparse categorical crossentropy' como la función de pérdida para un problema de
# clasificación multiclase, y seguirá la 'precisión' o 'accuracy' como métrica
# para evaluar el rendimiento del modelo.

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Entrenar el modelo
# train_images, train_labels:

# train_images contiene las imágenes de entrenamiento. Estas son los datos que el
# modelo usará para aprender.
# train_labels contiene las etiquetas correspondientes a train_images. Estas e
# tiquetas son la "respuesta correcta" o el resultado que el modelo intentará
# predecir. En un escenario de clasificación, por ejemplo, cada imagen tendría una
# etiqueta que indica su categoría.

# epochs=3:
# Un 'epoch' es una iteración completa sobre todo el conjunto de datos de entrenamiento.
# epochs=3 significa que el conjunto completo de datos de entrenamiento se pasará
# por el modelo un total de tres veces. Cada epoch permite que el modelo aprenda
# más sobre los datos, pero más epochs también pueden llevar a un sobreajuste,
# donde el modelo se ajusta demasiado a los datos de entrenamiento y no generaliza
# bien a nuevos datos.

# validation_split=0.2:
# Esta opción divide los datos de entrenamiento en dos partes: una para el
# entrenamiento real y otra para la validación.
# 0.2 significa que el 20% de los datos de entrenamiento se separan para la
# validación. Esto no se utiliza para entrenar el modelo, sino para evaluar su
# rendimiento en datos que no ha visto durante el entrenamiento. Es una buena
# práctica para comprobar si hay sobreajuste.

model.fit(train_images, train_labels, epochs=3, validation_split=0.2)

# Diferencias entre loss/val_loss y entre accuracy/val_accuracy

# Loss y val_loss:

# Loss: Medida del error del modelo en los datos de entrenamiento.
# val_loss: Medida del error del modelo en los datos de validación.
# La diferencia entre ambos indica si el modelo está "memorizando" los datos de
# entrenamiento y no generaliza bien a datos nuevos (sobreajuste).

# Accuracy y val_accuracy:

# Accuracy: Porcentaje de predicciones correctas en los datos de entrenamiento.
# val_accuracy: Porcentaje de predicciones correctas en los datos de validación.
# Similar a loss/val_loss, val_accuracy refleja la capacidad de generalización del
# modelo.

# En resumen:

# Loss y accuracy: Miden el aprendizaje del modelo en los datos de entrenamiento.
# val_loss y val_accuracy: Evaluan la capacidad de generalización del modelo a
# datos nuevos.
# Tu ejemplo (Epoch 3/3):

# Los valores indican que el modelo se comporta bien tanto en entrenamiento como
# en validación.
# La pequeña diferencia entre loss/val_loss y accuracy/val_accuracy sugiere que el
# modelo generaliza bien y no hay sobreajuste significativo.

# Consejos:

# Busca que loss y val_loss disminuyan con la formación.
# Grandes diferencias entre loss/val_loss o accuracy/val_accuracy pueden indicar
# sobreajuste.
# Ajusta los hiperparámetros (tasa de aprendizaje, épocas, etc.) para mejorar el
# rendimiento del modelo.

# Evaluar el modelo
# model.evaluate(...):

# model es tu modelo de aprendizaje automático, que ya ha sido entrenado con datos
# de entrenamiento.
# .evaluate() es un método que se utiliza para evaluar el rendimiento del modelo
# en un conjunto de datos separado, en este caso, los datos de prueba.
# test_images, test_labels:

# test_images contiene las imágenes de prueba. Estas son datos nuevos que el modelo
# no ha visto durante su entrenamiento y se utilizan para evaluar qué tan bien el
# modelo generaliza a datos no vistos.
# test_labels contiene las etiquetas correspondientes a test_images. Estas etiquetas
# son la "respuesta correcta" para cada imagen de prueba.
# test_loss, test_acc = model.evaluate(...):

# Cuando ejecutas model.evaluate(), devuelve dos valores: la pérdida de prueba
# (test_loss) y la precisión de prueba (test_acc).
# test_loss es el valor de la función de pérdida del modelo en el conjunto de datos
# de prueba, que mide cuánto se equivoca el modelo en promedio.
# test_acc es la precisión del modelo en el conjunto de datos de prueba, que indica
# el porcentaje de etiquetas que el modelo predijo correctamente.

test_loss, test_acc=model.evaluate(test_images, test_labels)
print('\nTest accuracy:', test_acc)

"""# 6 - RED NEURONAL. DATASET ABALONE (MOLUSCOS)"""

import pandas as pd

# Cargar el dataset

df = pd.read_csv('abalone.csv')

# Mostrar las primeras filas del dataset para explorarlo
df.sample(15)

import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Convertir las columnas categóricas en numéricas
df = pd.get_dummies(df, columns=['Sex'])

# Separar las características y las etiquetas
X = df.drop('Rings', axis=1)
y = df['Rings']

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Escalar las características
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Definir el modelo
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(64, activation='relu'),
    Dense(1)
])

# Compilar el modelo
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

model.summary()

# Entrenar el modelo
history = model.fit(X_train, y_train, epochs=100, validation_split=0.2)

# Evaluar el modelo
loss, mae = model.evaluate(X_test, y_test)
print(f'Mean Absolute Error: {mae}')

# Términos Clave
# Loss (Pérdida): Es una medida de qué tan bien está funcionando el modelo. Para
# problemas de regresión como este, la pérdida generalmente se calcula usando el
# error cuadrático medio (MSE). Es una suma de los errores al cuadrado entre las
# predicciones del modelo y los valores reales. Un valor más bajo indica un mejor
# rendimiento del modelo.

# MAE (Error Absoluto Medio): Es la media de las diferencias absolutas entre las
# predicciones y los valores reales. Nos dice, en promedio, cuántos anillos de
# diferencia hay entre las predicciones del modelo y los valores reales.

# Val_loss y val_mae: Son las métricas de pérdida y error absoluto medio calculadas
# en el conjunto de datos de validación. Nos ayudan a ver cómo de bien se está
# generalizando el modelo a datos que no ha visto durante el entrenamiento.

# En la primera época, la pérdida y el error absoluto medio son bastante altos
# tanto para el conjunto de entrenamiento como para el de validación. Esto es
# normal ya que el modelo está comenzando a aprender.

# En la segunda época, hay una mejora significativa en la pérdida y el error
# absoluto medio. El modelo está empezando a aprender patrones útiles de los datos.

# Para la décima época, la pérdida y el error absoluto medio siguen disminuyendo,
# lo que indica que el modelo está mejorando su capacidad de predicción.

# A mitad del entrenamiento, la pérdida y el error absoluto medio han disminuido
# considerablemente en comparación con las primeras épocas. Esto muestra una buena
# tendencia de aprendizaje.

# Al final del entrenamiento, la pérdida y el error absoluto medio son bastante
# bajos, tanto para el conjunto de entrenamiento como para el de validación. Esto
# sugiere que el modelo ha aprendido bien a partir de los datos.

# La evaluación final del modelo en el conjunto de prueba muestra un MAE de
# aproximadamente 1.5, lo que indica que, en promedio, las predicciones del
# modelo están a 1.5 anillos de distancia de las edades reales de los moluscos.

# Conclusiones
# El modelo ha mejorado significativamente desde el comienzo del entrenamiento,
# con una reducción consistente en la pérdida y el MAE.
# La pérdida y el MAE en los conjuntos de entrenamiento y validación son bastante
# similares, lo que sugiere que no hay un sobreajuste significativo.
# El modelo parece estar generalizando bien, con un MAE final de alrededor de 1.5
# en el conjunto de prueba, lo cual es razonable para este tipo de problema.

import matplotlib.pyplot as plt

# Visualizar la pérdida durante el entrenamiento
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""#7 - RED NEURONAL. DATASET CALIFORNIA HOUSING CLEAN"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Cargar los datos
housing_data = pd.read_csv('california housing clean.csv')

# Eliminar filas con valores faltantes
housing_data.dropna(inplace=True)

# Codificar la variable categórica 'ocean_proximity' utilizando el método básico de pandas
housing_data['ocean_proximity'] = housing_data['ocean_proximity'].astype('category').cat.codes

# Separar características y variable objetivo
X = housing_data.drop('median_house_value', axis=1)
y = housing_data['median_house_value']

# Normalizar las características numéricas
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Definir el modelo
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1)
])

# Compilar el modelo
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Entrenar el modelo
history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=32, verbose=1)

# Evaluar el modelo en el conjunto de prueba
test_loss, test_mae = model.evaluate(X_test, y_test, verbose=2)

print(f'Error Cuadrático Medio en el conjunto de prueba: {test_loss}')
print(f'Error Absoluto Medio en el conjunto de prueba: {test_mae}')

"""#8 - CHATBOT. ARQUITECTURA TRANSFORMERS. LIBRERIA HUGGINGFACE

Transformers es una arquitectura de deep learning introducida en 2017. Esta arquitectura se basa en un mecanismo de atención que permite al modelo enfocarse en diferentes partes del input para generar la output. Los transformers han revolucionado el campo del procesamiento del lenguaje natural (NLP) debido a su capacidad para manejar secuencias de datos de manera más efectiva que los modelos anteriores, como las redes neuronales recurrentes (RNNs) y las redes de memoria a largo plazo (LSTM).

Características principales:

- Atención: Permite al modelo enfocarse en las partes más relevantes del input.

- Paralelismo: A diferencia de los RNNs, los transformers permiten un procesamiento paralelo, lo que acelera el entrenamiento y la inferencia.

- Escalabilidad: Han demostrado ser altamente escalables, llevando a la creación de modelos gigantes como GPT-3.

1.- IMPORTAR LAS BIBLIOTECAS

IMportar AutoTokenizer:

AutoTokenizer es una clase de la biblioteca transformers de Hugging Face. Se utiliza para cargar el tokenizador adecuado para el modelo especificado. Un tokenizador convierte texto en números (tokens) que el modelo puede procesar.

Importar BlenderbotForConditionalGeneration:

Función principal: Este es un modelo de chatbot preentrenado que sabe generar respuestas basadas en una entrada de texto específica.
Cómo funciona: Piensa en este modelo como un generador de texto que puede continuar una conversación. Cuando le das una frase, usa su entrenamiento previo para producir una respuesta adecuada y coherente.

Importar pipeline:

Función principal: pipeline es una función sencilla que permite utilizar modelos preentrenados para tareas comunes sin necesidad de mucha configuración.
Cómo funciona: Simplifica el proceso de usar modelos para varias tareas, como responder preguntas, traducir textos, o en este caso, mantener conversaciones. Al usar pipeline, no necesitas preocuparte por los detalles técnicos de cómo cargar y usar el modelo.
"""

# Importar las bibliotecas necesarias
from transformers import AutoTokenizer, BlenderbotForConditionalGeneration, pipeline

"""2.-CREAR LAS CANALIZACIONES (PIPELINES) PARA TRADUCCION:

pipeline es una función de la biblioteca transformers de Hugging Face que facilita el uso de modelos preentrenados para tareas comunes de procesamiento de lenguaje natural, como traducción, generación de texto, clasificación, entre otras.

Crear un traductor de inglés a español:

traductor_ingles_espanol = pipeline("translation", model="Helsinki-NLP/opus-mt-en-es")

Esto crea un traductor que convierte texto de inglés a español. Utiliza un modelo específico (Helsinki-NLP/opus-mt-en-es) preentrenado en millones de textos para esta tarea de traducción.
Cuando se le da una frase en inglés, este traductor devolverá la frase traducida al español.


"""

traductor_ingles_espanol = pipeline("translation", model="Helsinki-NLP/opus-mt-en-es")
traductor_espanol_ingles = pipeline("translation", model="Helsinki-NLP/opus-mt-es-en")

"""3.- CARGAR EL MODELO  Y EL TOKENIZADOR PREENTRENADOS DE BLENDERBOT

Definir el nombre del modelo:

mname = "facebook/blenderbot-400M-distill"
Aquí se está guardando el nombre del modelo preentrenado BlenderBot 400M de Facebook en la variable mname.

model = BlenderbotForConditionalGeneration.from_pretrained(mname)
Se carga el modelo BlenderBot 400M utilizando la clase BlenderbotForConditionalGeneration y el nombre del modelo almacenado en mname. Este modelo se utiliza para generar respuestas condicionales, es decir, respuestas basadas en una entrada específica.

Resumen

mname: Nombre del modelo preentrenado.

model: Modelo preentrenado cargado, usado para generar texto.

tokenizer: Tokenizador correspondiente al modelo, usado para convertir texto en tokens y viceversa.

"""

mname = "facebook/blenderbot-400M-distill"
model = BlenderbotForConditionalGeneration.from_pretrained(mname)
tokenizer = AutoTokenizer.from_pretrained(mname)

"""4.- CREAR DIFERENTES TIPOS DE DESPEDIDA PARA TERMINAR EL CHAT"""

mensajes_despedida = [
    "goodbye", "see you later", "farewell", "until next time", "take care",
    "adios", "hasta luego", "have a great day", "hasta la vista", "catch you later",
    "bye-bye", "it was nice talking to you", "until we meet again", "stay well",
    "peace out", "later gator", "toodles", "ciao", "auf wiedersehen", "salir", "go out", "exit", "bye"
]

"""5.- DEFINIR LA FUNCION generar_respuesta()

Explicación:

1.0) Definición de la función:

def generar_respuesta(entrada_usuario): Se define una función llamada generar_respuesta que recibe un parámetro de entrada llamado entrada_usuario.

1.1) Traducción de la entrada del usuario:

entrada_ingles = traductor_espanol_ingles(entrada_usuario)[0]["translation_text"]: Se utiliza la variable traductor_espanol_ingles para traducir la entrada del usuario al inglés. La traducción se guarda en la variable entrada_ingles.

1.2) Conversión a formato compatible con el modelo:

inputs = tokenizer([entrada_ingles], return_tensors="pt"):

Se utiliza el tokenizador tokenizer para convertir la entrada en inglés a un formato que el modelo model pueda entender (tensores de Pytorch). El resultado se guarda en la variable inputs.

1.3) Generación de la respuesta:

reply_ids = model.generate(**inputs): Se utiliza el modelo model para generar una respuesta basada en la entrada del usuario. La respuesta se guarda en la variable reply_ids. Los IDs son los DNIs de cada token

1.4) Traducción de la respuesta al español:

respuesta_espanol = traductor_ingles_espanol(tokenizer.batch_decode(reply_ids, skip_special_tokens=True))[0]["translation_text"]:

Se utiliza la variable traductor_ingles_espanol para traducir la respuesta del modelo al español. La traducción se guarda en la variable respuesta_espanol.

La función batch_decode es parte del módulo transformers de Hugging Face y se utiliza para convertir listas de IDs de tokens de vuelta a texto. Es particularmente útil cuando se trabaja con lotes (batch) de secuencias de tokens.

¿Qué hace batch_decode?
Propósito: Convierte una lista de listas de IDs de tokens (donde cada lista representa una secuencia) en una lista de cadenas de texto.
Entrada: Una lista de secuencias de IDs de tokens.
Salida: Una lista de cadenas de texto correspondientes a las secuencias de tokens.

1.5) Retorno de la respuesta:

return respuesta_espanol: Se devuelve la variable respuesta_espanol como resultado de la función.

"""

# Definir la función `generar_respuesta()`
def generar_respuesta(entrada_usuario):
    # Traducir la entrada del usuario al inglés
    entrada_ingles = traductor_espanol_ingles(entrada_usuario)[0]["translation_text"]

    # Convertir la entrada del usuario en un formato que el modelo pueda entender
    inputs = tokenizer([entrada_ingles], return_tensors="pt")

    # Generar una respuesta basada en la entrada del usuario
    reply_ids = model.generate(**inputs)

    # Traducir la respuesta del modelo de nuevo al español
    respuesta_espanol = traductor_ingles_espanol(tokenizer.batch_decode(reply_ids,
                            skip_special_tokens=True))[0]["translation_text"]

    return respuesta_espanol

"""6.- SALUDAR AL USUARIO"""

print("Hola, soy un bot que puede participar en conversaciones casuales y un poco más")

"""7.-BUCLE DE LA CONVERSACION

1.0) Inicio del bucle:

while True: Se inicia un bucle infinito que se ejecutará hasta que se cumpla una condición de parada.

1.1) Solicitud de entrada al usuario:

entrada_usuario = input("Usuario: "): Se solicita al usuario que introduzca un mensaje. La entrada del usuario se guarda en la variable entrada_usuario.

1.2) Comprobación de palabras clave de despedida:

if entrada_usuario.lower() in mensajes_despedida: Se comprueba si la entrada del usuario en minúsculas está presente en la lista mensajes_despedida. Si se encuentra una palabra clave de despedida, se ejecuta el siguiente código:

1.3) Salida del bucle y despedida:

print('Bot: Adiós'): Se imprime un mensaje de despedida por parte del bot. Se utiliza la función break para salir del bucle while True.

1.4) Generación de la respuesta:

respuesta = generar_respuesta(entrada_usuario): Se llama a la función generar_respuesta con la entrada del usuario como parámetro. La respuesta del modelo se guarda en la variable respuesta.

1.5) Impresión de la respuesta:

print(f"Bot: {respuesta}"): Se imprime la respuesta del modelo con el prefijo "Bot:
"""

# Bucle de la conversación
while True:
    # Solicitar la entrada del usuario en español
    entrada_usuario = input("Usuario: ")

    # Terminar la conversación si el usuario escribe una palabra clave de despedida
    if entrada_usuario.lower() in mensajes_despedida:
        print('Bot: Adiós')
        break

    # Generar la respuesta del modelo y traducirla al español
    respuesta = generar_respuesta(entrada_usuario)

    # Imprimir la respuesta del bot en español
    print(f"Bot: {respuesta}")